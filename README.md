# SPADE-pix2pix-for-Anime
This pix2pix model is based on Generative Adversarial Networks (GAN, GANs). I implemented in order to generate from a segmentation label to an anime illust. I could not get a satisfactory result, but obtained enough for small and rough datasets.   
<br>

## Architecture  
- The original `SPADE` computes a weight from label at each itself layers, so this architecture has a mapping network like the StyleGAN and precompute weights. However, it is `Constant-Resolution FCN`, uses `AtrousConvolution`, instead of `FCN` in order not to do down sampling. In addition, each `SPADE` layer resize and share a encoded weight by a mapping network.  
- I want to assign a hair color, so this Generator has double mapping networks, inputed a segmented label to one mapping network and a RGB color map, to another. The RGB color map has a hair color and positions, it is same resolution with label.  
- I added `NoiseAdder` like StyleGAN.  
- I selected a single patch discriminator with `SelfAttention` instead of `Multi-scale discriminator`.  
- The discriminator's loss is `Hinge loss` and `Zero Centered Gradient Penalty`.  
- The generator's loss is `Hinge loss`, `Feature Matching loss` and `Perceptual loss`.  

<br>

## Result  
<img src="https://github.com/pit-ray/SPADE-pix2pix-for-Anime/blob/master/tile.png?raw=true" width=512>  

This result is obtained by training with **500 pixiv images**.  That is very small datasets.  
Additionaly, test datasets is generated by <a href="https://github.com/pit-ray/Anime-Semantic-Segmentation-GAN">Anime-Semantic-Segmentation-GAN</a>, and training datasets is manualy anotated. However, I cannot publish training datasets for copyright of source images.  

The parameters of the upper result is almost same as default value of `options.py`.  
<br>

## How to train
Please create `dataset` directory and prepare dataset. Next, you can set dataset path to option of command.<br>
<br>
```  
Python3 train.py --dataset_dir dataset/example
```  
<br>  

## Pretrained Weights  
Shold you want them, please send to me by Issues. Then I am going to re-train by my datasets and appear download URL, because pre-trained weights don't match refactored python-scripts.  

<br>  

## Environment
||details|
|---|---|
|OS|Windows10 Home|
|CPU|AMD Ryzen 2600|
|GPU|MSI GTX 960 4GB|
|language|Python 3.7.1|
|framework|Chainer 7.0.0, cupy-cuda91 5.3.0|

<br>

## References  
[1] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu. Semantic Image Synthesis with Spatially-Adaptive Normalization. <i>arXiv preprint  <a href="https://arxiv.org/abs/1903.07291">arXiv:1903.07291, 2019</a></i>  

[2] Tero Karras, Samuli Laine, Timo Aila. A Style-Based Generator Architecture for Generative Adversarial Networks
. <i>arXiv preprint <a href="https://arxiv.org/abs/1812.04948">arXiv:1812.04948, 2019</a></i>  

[3] Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena.  Self-Attention Generative Adversarial Networks. <i>arXiv preprint <a href="https://arxiv.org/abs/1805.08318">arXiv:1805.08318, 2019</a></i>  

[4] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. Generative Adversarial Networks. <i>arXiv preprint  <a href="https://arxiv.org/abs/1406.2661">arXiv:1406.2661, 2014</a></i>

[5] Jonathan Long, Evan Shelhamer, Trevor Darrell. Fully Convolutional Networks for Semantic Segmentation. <i>arXiv preprint <a href="https://arxiv.org/abs/1411.4038">arXiv:1411.4038, 2015</a></i>

[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam. Rethinking Atrous Convolution for Semantic Image Segmentation. <i>arXiv preprint <a href="https://arxiv.org/abs/1706.05587">arXiv:1706.05587, 2017 (v3)</a></i>

[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. <i>arXiv preprint <a href="https://arxiv.org/abs/1606.00915">arXiv:1606.00915, 2017 (v2)</a></i>

[6] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida. Spectral Normalization for Generative Adversarial Networks. <i>arXiv preprint <a href="https://arxiv.org/abs/1802.05957">arXiv:1802.05957, 2018</a></i>

[7] Wenzhe Shi, Jose Caballero, Ferenc Husz√°r, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, Zehan Wang. Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network. <i>arXiv preprint <a href="https://arxiv.org/abs/1609.05158">arXiv:1609.05158, 2016</a></i>

[8] Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, Ming-Hsuan Yang. Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis. <i>arXiv preprint <a href="https://arxiv.org/abs/1903.05628">arXiv:1903.05628, 2019(v6)</a></i>  

[9] Lars Mescheder, Andreas Geiger, Sebastian Nowozin. Which Training Methods for GANs do actually Converge?. <i>arXiv preprint <a href="https://arxiv.org/abs/1801.04406">arXiv:1801.04406, 2018</a></i>  

[10] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. <i>arXiv preprint <a href="https://arxiv.org/abs/1711.11585">arXiv:1711.11585, 2018</a></i>  

[11] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros. Image-to-Image Translation with Conditional Adversarial Networks. <i>arXiv preprint <a href="https://arxiv.org/abs/1611.07004">arXiv:1611.07004, 2018</a></i>  


## Author  
- pit-ray  
[E-mail] contact(at)pit-ray.com
